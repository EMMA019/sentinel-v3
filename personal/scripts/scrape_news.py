#!/usr/bin/env python3
"""
scripts/scrape_news.py â€” è¤‡æ•°ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åŽé›†
====================================================
BeautifulSoup4 + requests ã§ä»¥ä¸‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°:
- Seeking Alpha
- Yahoo Finance
- Benzinga
- FMP News APIï¼ˆæ—¢å­˜ï¼‰

ä½¿ã„æ–¹:
  python scripts/scrape_news.py NVDA
  â†’ nvda_news.json ã‚’å‡ºåŠ›
"""
import os, json, sys, time
from pathlib import Path
from datetime import datetime
import requests
from bs4 import BeautifulSoup

sys.path.append(str(Path(__file__).parent.parent / "shared"))
from engines import core_fmp

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}


def scrape_seeking_alpha(ticker: str) -> list:
    """Seeking Alpha ã®æœ€æ–°è¨˜äº‹"""
    print(f"  ðŸ“° Seeking Alpha...")
    try:
        url = f"https://seekingalpha.com/symbol/{ticker}/news"
        resp = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(resp.content, 'html.parser')
        
        articles = []
        # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’æŽ¢ã™ï¼ˆã‚µã‚¤ãƒˆæ§‹é€ ã«ä¾å­˜ï¼‰
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if '/article/' in href and not href.startswith('http'):
                href = 'https://seekingalpha.com' + href
            
            title = link.get_text(strip=True)
            if title and len(title) > 10 and href not in [a['url'] for a in articles]:
                articles.append({
                    'source': 'Seeking Alpha',
                    'title':  title,
                    'url':    href,
                    'date':   datetime.now().strftime('%Y-%m-%d'),
                })
            
            if len(articles) >= 5:
                break
        
        return articles
    except Exception as e:
        print(f"    âš ï¸  Seeking Alpha ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def scrape_yahoo_finance(ticker: str) -> list:
    """Yahoo Finance ãƒ‹ãƒ¥ãƒ¼ã‚¹"""
    print(f"  ðŸ“° Yahoo Finance...")
    try:
        url = f"https://finance.yahoo.com/quote/{ticker}/news"
        resp = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(resp.content, 'html.parser')
        
        articles = []
        # h3ã‚¿ã‚°ã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŽ¢ã™
        for h3 in soup.find_all('h3'):
            link = h3.find('a', href=True)
            if link:
                title = link.get_text(strip=True)
                href  = link['href']
                if not href.startswith('http'):
                    href = 'https://finance.yahoo.com' + href
                
                if title and len(title) > 10:
                    articles.append({
                        'source': 'Yahoo Finance',
                        'title':  title,
                        'url':    href,
                        'date':   datetime.now().strftime('%Y-%m-%d'),
                    })
            
            if len(articles) >= 5:
                break
        
        return articles
    except Exception as e:
        print(f"    âš ï¸  Yahoo Finance ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def scrape_benzinga(ticker: str) -> list:
    """Benzinga ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆRSSã¾ãŸã¯HTMLï¼‰"""
    print(f"  ðŸ“° Benzinga...")
    try:
        # Benzingaã¯ä¼šå“¡åˆ¶ãŒå¤šã„ã®ã§ã€RSSã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
        url = f"https://www.benzinga.com/stock/{ticker}"
        resp = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(resp.content, 'html.parser')
        
        articles = []
        for link in soup.find_all('a', href=True, class_=lambda c: c and 'title' in c.lower()):
            title = link.get_text(strip=True)
            href  = link['href']
            if not href.startswith('http'):
                href = 'https://www.benzinga.com' + href
            
            if title and len(title) > 10 and href not in [a['url'] for a in articles]:
                articles.append({
                    'source': 'Benzinga',
                    'title':  title,
                    'url':    href,
                    'date':   datetime.now().strftime('%Y-%m-%d'),
                })
            
            if len(articles) >= 5:
                break
        
        return articles
    except Exception as e:
        print(f"    âš ï¸  Benzinga ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def get_fmp_news(ticker: str) -> list:
    """FMP News APIï¼ˆæ—¢å­˜ï¼‰"""
    print(f"  ðŸ“° FMP API...")
    try:
        news = core_fmp.get_news(ticker, limit=10)
        return [{
            'source': 'FMP',
            'title':  n['title'],
            'url':    n['url'],
            'date':   n['published_at'][:10],
            'text':   n.get('text', '')[:200],
        } for n in (news or [])]
    except Exception as e:
        print(f"    âš ï¸  FMP ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def sentiment_analysis(articles: list) -> dict:
    """ç°¡æ˜“ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æž"""
    positive_words = ['beat', 'upgrade', 'buy', 'strong', 'growth', 'surge', 'bullish', 'outperform']
    negative_words = ['miss', 'downgrade', 'sell', 'weak', 'decline', 'drop', 'bearish', 'underperform']
    
    pos_count = 0
    neg_count = 0
    
    for a in articles:
        text = (a.get('title', '') + ' ' + a.get('text', '')).lower()
        pos_count += sum(1 for w in positive_words if w in text)
        neg_count += sum(1 for w in negative_words if w in text)
    
    total = pos_count + neg_count
    if total == 0:
        return {'score': 0, 'label': 'Neutral'}
    
    score = (pos_count - neg_count) / total * 100
    if score > 30:
        label = 'Bullish'
    elif score < -30:
        label = 'Bearish'
    else:
        label = 'Neutral'
    
    return {
        'score': round(score, 1),
        'label': label,
        'positive_count': pos_count,
        'negative_count': neg_count,
    }


def main():
    if len(sys.argv) < 2:
        print("Usage: python scrape_news.py TICKER")
        sys.exit(1)
    
    ticker = sys.argv[1].upper()
    print(f"=== News Scraper: {ticker} ===")
    
    # å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰åŽé›†
    all_articles = []
    all_articles.extend(get_fmp_news(ticker))
    time.sleep(1)
    all_articles.extend(scrape_seeking_alpha(ticker))
    time.sleep(1)
    all_articles.extend(scrape_yahoo_finance(ticker))
    time.sleep(1)
    all_articles.extend(scrape_benzinga(ticker))
    
    # é‡è¤‡å‰Šé™¤
    unique = []
    seen_urls = set()
    for a in all_articles:
        if a['url'] not in seen_urls:
            unique.append(a)
            seen_urls.add(a['url'])
    
    # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æž
    sentiment = sentiment_analysis(unique)
    
    # çµæžœ
    print(f"\n{'='*60}")
    print(f"åŽé›†ä»¶æ•°: {len(unique)}ä»¶")
    print(f"ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ: {sentiment['label']} ({sentiment['score']:+.1f})")
    print(f"  ãƒã‚¸ãƒ†ã‚£ãƒ–: {sentiment['positive_count']} / ãƒã‚¬ãƒ†ã‚£ãƒ–: {sentiment['negative_count']}")
    print(f"{'='*60}")
    
    for i, a in enumerate(unique[:10], 1):
        print(f"{i}. [{a['source']}] {a['title'][:60]}...")
    
    # JSONä¿å­˜
    out = {
        "generated_at": datetime.now().isoformat(),
        "ticker":       ticker,
        "total_count":  len(unique),
        "sentiment":    sentiment,
        "articles":     unique,
    }
    
    out_file = Path(__file__).parent.parent / "frontend" / "public" / "content" / f"{ticker.lower()}_news.json"
    out_file.write_text(json.dumps(out, indent=2, ensure_ascii=False))
    print(f"\nâœ… Saved: {out_file}")


if __name__ == "__main__":
    main()
